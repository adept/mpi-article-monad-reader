\documentclass{tmr}

\usepackage{mflogo}

%include lhs2TeX.fmt
%include lhs2TeX.sty
%include polycode.fmt

\title{High Performance Haskell with MPI}
\author{Bernie Pope\email{bjpope@unimelb.edu.au}}
\author{Dmitry Astapov\email{dastapov@gmail.com}}

\newcommand{\Todo}[1]{{\textbf{Todo: #1}}}

\begin{document}

\begin{introduction} 
What is MPI, what is haskell-mpi, how can you use it for writing multi-node programs in Haskell or interoperate with other languages
\end{introduction}

\section{Distributed-memory parallelism and MPI}

The World's largest supercomputers now feature hundreds of thousands of CPU cores, and mega-core machines
are just around the corner.\footnote{As one example, the Lawrence Livermore National Laboratory
is preparing to install Sequoia, a 1.6 million core IBM BlueGene/Q (\url{https://asc.llnl.gov/computing_resources/sequoia/}).} There are many technical
challenges to building such behemoths, not the least of which is
the CPU-to-memory bottleneck. Shared memory parallel computers --- which now dominate consumer-grade
systems --- provide the convenient abstraction that every processor has equivalent access to all memory,
even though the underlying connections between processors and memory are typically non-uniform.
Unfortunately (and unsurprisingly) it is difficult to scale this abstraction in a
cost-effective way beyond a few thousand cores. A quick glance at the Top 500 list of supercomputers reveals
no significant large-scale shared-memory systems in recent years.\footnote{\url{http://www.top500.org/}}
Instead we see that practically all the listed supercomputers are based on
distributed-memory parallelism. These machines are built from many independent computers (called nodes),
each with their own processors, memory and operating system, interconnected by one or more high-speed networks.
The nodes themselves are often smaller multi-core shared-memory computers, nevertheless
the macro architecture is distributed.

Distributed-memory parallelism does not really solve the CPU-to-memory bottleneck (over the whole machine),
after all, sharing data between nodes over a network is a relatively costly operation.
Instead it forces programmers to
address the non-uniformity head on, which typically means adopting an explicitly distributed style of
parallel programming, and often requires new algorithms to be devised.

The Message Passing Interface (MPI) is a ``message-passing library interface specification''
for writing distributed-parallel programs~\cite{mpi-report}. Various realizations of the
specification are provided by software libraries, some of which are open source
(such as OpenMPI\footnote{\url{http://www.open-mpi.org/}} and MPICH\footnote{\url{http://www.mcs.anl.gov/research/projects/mpich2/}}), and some of which are proprietary.
As the name suggests, MPI is firmly rooted in the paradigm of message passing.
An MPI application consists of numerous independent computing processes (or tasks)
which collaborate by sending messages amongst themselves.
The underlying communication protocols are programming language agnostic, but standard APIs are
defined for Fortran, C and C++. Bindings in other languages, such as Haskell-MPI,
are typically based on foreign interfaces to the C API.

Haskell-MPI provides a fairly modest wrapping of MPI, and is guided by two objectives:
\begin{enumerate}
 \item Convenience: for a small cost, it should be easy to send arbitrary (serialisable)
data structures as messages.
 \item Performance: low overhead communications should be possible,
particularly for array-like data structures.
\end{enumerate}
It is difficult to satisfy both objectives in one implementation, so Haskell-MPI provides two interfaces.
The first is simple to use (more automated) but potentially slower (more data copying), the second
is more cumbersome to use (less automated) but potentially faster (less data copying).

This article aims give you a taste of distributed parallel programming with Haskell-MPI, enough to
whet your appetite, without getting too bogged down in details. Those who find themselves hungry for
more can consult the haddock pages for a more substantial course and check out examples in the package sources.

We begin by introducing the technique of computing definite integrals by the trapezoid method.
This lends itself to an easy-to-parallelise algorithm which will serve as the
basis of programming examples in the following sections. We take a simple sequential implementation
of the algorithm, and morph it into two different parallel implementations.
The first uses shared-memory and threads, and the second uses distributed-memory and Haskell-MPI.
To see how well we fare against the conventional school, we also provide an MPI implementation in C.
We then evaluate the performance of each version on a non-trivial problem instance and compare the results.
We conclude by discussing some of the more interesting features of Haskell-MPI that were not covered in
previous sections, and show how Haskell-MPI and C-MPI can coexist in a single parallel computation.

%\section{Very short intro to MPI and haskell-mpi and outlining the motivation}
%What is MPI? If you ever tried to make processes running on separate
%computers talk to each other, chances are that you already came across
%the MPI or invented parts of it. 

%Even though MPI has been standartized only aroung 1995\footnote{Full
%  formal definition of protocol, its semantics and reference
%  implementation could be found in \cite{mpi-report}}, it has become
%de-facto standart in programming on architecures with distributed
%memory (``clusters'' or ``parallel computers''). It tries to prove a
%language-independent communications means that would allow users to
%pass around ``messages'' (hence the name, ``Message Passing
%Interface''). We have it on a good authority that MPI is still widely
%used today, even though there are numerous alternatives for
%inter-process communication. This article would focus on haskell-mpi,
%not trying to provide any sort of comparison with possible
%alternatives, as authors believe that any choice like this should
%ultimately be made in context of the specific task at hand.

%So what is, then, haskell-mpi? It is a package of low- and high-level bindings
%to C MPI library that allows Haskell programmers to call upon the
%powers of MPI with ease. 

%What would be the most (stereo)typical use case for haskell-mpi?
%Suppose that you have a task which easily lends itself to subdivision
%into a number of independent subtasks. In this day and age, when
%having multiple networked computers in vicinity is more of a rule than
%an exception, it is only natural to try and execute all the subtasks
%on different machines in parallel and then combine their results.

\section{Computing definite integrals with trapezoids}

\begin{figure*}[t]
\centering
\includegraphics[width=8cm]{integral_diag.pdf}
\caption{Approximating $\int_{x=a}^{b} f(x)$ by summing the area of three trapezoids.
\label{trapezoidfig}}
\end{figure*}

We now consider the problem of computing definite integrals
using the trapezoid method. The algorithm is naturally data parallel, and is a common
introductory example in parallel programming tutorials. Our presentation is inspired by
Pacheo's textbook on MPI~\cite{Pacheo}.
We can approximate integrals by summing the area of a consecutive
trapezoids lying under a function within a given interval, as illustrated in
Figure~\ref{trapezoidfig}. A single trapezoid spanning the interval
$[x_0,x_1]$ has area $(x_1 - x_0)(f(x_0) + f(x_1))/2$. Extending this to $n$ equally spaced subintervals $[x_0,x_1,\ldots,x_n]$
we arrive at the formula:
\begin{equation*}
\begin{split}
\int_{x=a}^{b} f(x)\ \approx\ &\ h \Bigl(f(x_0)/2 + f(x_1) + \ldots f(x_{n-1}) + f(x_n)/2\Bigr) \\
                     &\ \textrm{where} \\
                     &\ x_0\ =\ a,\ x_n\ =\ b\\
                     &\ h\ =\ (a - b) / n\\
                     &\ \forall i \in \{0 \ldots n-1\},\ x_{i+1} - x_{i}\ =\ h
\end{split}
\end{equation*}

% ./code/Trapezoid.hs
\begin{listing}
\begin{Verbatim}
module Main where

import System (getArgs)

main :: IO ()
main = do
  aStr:bStr:nStr:_ <- getArgs
  let [a,b] = map read [aStr,bStr]
      n = read nStr
      h = (b - a) / fromIntegral n
      integral = trapezoid f a b n h
  print integral 

-- Integrate f on interval [a,b] using n steps of size h
trapezoid :: (Double -> Double) ->
             Double -> Double -> Int -> Double -> Double
trapezoid f a b n h =
  h * sum (endPoints:internals)
  where
  endPoints = (f a + f b) / 2
  internals = map f $ take (n - 1) $ iterate (+h) (a + h)

f :: Double -> Double
f x = 4 / (1 + x * x)
\end{Verbatim}
\end{listing}

After splitting interval into arbitrary number of parts you could
integrate them separately and them sum the results. In fact, very
simple modification is required to make this code use all available
cores in single multi-cored machine:
%% ./code/Trapezoid-threads.hs
\begin{Verbatim}
module Main where

import Control.Concurrent
import Control.Parallel.Strategies
import System (getArgs)
import Data.List

main :: IO ()
main = do
  -- I tried this on ghc 6.10.4, which has no getNumCapabilities
  -- uncomment with later ghc's
  maxThreads <- return 4 -- getNumCapabilities
  aStr:bStr:nStr:_ <- getArgs
  let [a,b] = map read [aStr,bStr]
      n = read nStr
      h = (b - a) / fromIntegral n
      localN = n `div` fromIntegral maxThreads
      chunks = parMap rwhnf (\threadNo ->
         let localA = a + fromIntegral threadNo * fromIntegral localN * h
             localB = localA + fromIntegral localN * h
             in trapezoid f localA localB localN h) [0..maxThreads-1]
  print $ sum chunks

-- trapezoid and f defined as above
\end{Verbatim}
%% $

%% ideas for further sections: show how easy it would be to extend this to ``find number of nodes AND number of cores on them, split work according to power of node''

\section{Introducing haskell-mpi}

But what if you want to do that on more that one box to utilise even
bigger number of cores? You would have, at the very least, tell the
processes on each node which part of the task they would work on, and
later on you would have to gather and combine results from them.

This could be done manually, but if you want to run the program
repeatedly, or if your interprocess communications are even slightly
more complex, you would need some means of communication between
processes. In pseudo-code, this would look like this:

\begin{compactitem}
\item find number of available nodes
\item split interval between them
\item tell each node the interval it would work on
\item perform work (integration) on each node and send result to master node
\item collect results from nodes
\item sum them up
\end{compactitem}


With haskell-mpi this pseudo-code from the previous section could be
directly translated into Haskell:

\begin{Verbatim}
module Main where

import Control.Parallel.MPI.Simple
import System (getArgs)

main :: IO ()
-- TODO: maybe explicit commRank/commSize would be better here? Or
-- even further below, right besides the ``if'' ?
main = mpiWorld $ \numRanks rank -> do
  aStr:bStr:nStr:_ <- getArgs
  let [a,b] = map read [aStr,bStr]
      n = read nStr
      h = (b - a) / fromIntegral n
      localN = n `div` fromIntegral numRanks
      localA = a + fromIntegral rank * fromIntegral localN * h
      localB = localA + fromIntegral localN * h
      integral = trapezoid f localA localB localN h
  if rank == 0
     then print . sum =<< gatherRecv commWorld 0 integral
     else gatherSend commWorld 0 integral

-- trapezoid and f defined as above
\end{Verbatim}
%% $ - for naive auctex mode that thiks that math mode somehow escapes
%% from Verbatim and rules over the rest of the text. AuCTeX, you are wrong!

Here, \verb|commWorld| is a ``code name'' for the default group of processes
(``communicator'' in MPI terminology), which includes all the
processes participating in this particular run. Processes in the group
are numbered starting with zero. We've implemented
``many-to-one'' communication pattern, with process number 0
collecting data sent by all other processes (using \verb|gatherSend|)
and putting them into a list.

It might seem strange at first that processes do no exchange any
messages to determine how to divide the work among themselves. That is
because an established practice in MPI world is to run the same binary
on all computers (``nodes'') participating in the run, to reduce
development and deployment efforts. This way, if some run-time
parameters could be deduced solely from process number (``rank'') and
the information available to all processes at startup, it would not
require any additional message passing.

Thus, code that uses MPI usually features a substantial pieces of code,
which, while present in all binaries, are actually executed only in
some of the running copies. While programming with MPI in C, care must
be taken to avoid allocations/computations in processes that would not
actually use the appropriate data, and to avoid referencing/using
uninitialized data. Haskell, with its lazy evaluation and automatic
memory management, allows to significantly reduce amount of mental
energy required to get everything right. If computations are pure,
they simply would not be executed in the processes that don't use
them, without requiring any explicit ``housekeeping'' code.

\section{Does this really speeds things up?}

In order to compile and run our sample integration code, you have to
have \verb|haskell-mpi| package installed\footnote{Please refer to
  \ref{appendix-A}{Appendix A} for installation and running help}. After that,
simple \verb|ghc --make -O2 <file>.hs| should be sufficient to produce
a working MPI-enabled binary.

Even if you don't have several networked computers, you could still
run several processes on the same box, which actually makes sense if
you have multiple CPU cores. OpenMPI would do this by default if you
haven't configured your ``computing network'' topology. Try running
\verb|mpirun -n <N> ./ToDoNameHere ToDoArgsHere|, where \verb|N| is
from 1 up to the number of cores you have.

You should be able to observe almost linear speed up. We were able to
test this code on quite large computing cluster and got the following
numbers\footnote{runtimes from N tries, averaged}:

\Todo: measurement results here, table and graph. Measure simple version, threaded and MPI against each other

As you can see (describe something less obvious that could be derived
from the graph, for example that for large N computation takes less
time than transmission of the results).

\section{Fast API vs Simple API}

Haskell-mpi provides two different APIs suitable for different kinds
of data. Control.Parallel.MPI.Simple, which we used in our examples so
far, allows you to send just about any Haskell type over the wire, as
long as it is an instance of typeclass \verb|Serialize|. This
typeclass is defined in the package \verb|cereal|, which provides
instances for many popular Haskell types. It is expected that user
would write instances for user-defined types themselves, but
fortunately they are quite trivial.

To illustrate that, one of the authors took GitHub project McPhd,
which uses Haskell to simulate particles (\Todo: fix the desc,
copypaste from haskell para report here), and added support for
parallel execution of the simulation using MPI. GitHub patch ... shows
all the changes that were necessary to make this happen.


\section{Interacting with C}
You could use it to combine haskell and C code. Here is how: (separate sub-section on this?)

\begin{Verbatim}
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

double f(double);
double trapezoid(double, double, int, double);

int main(int argc, char **argv) {
   double a, b, h, local_a, local_b, integral, sum = 0;
   double *results = NULL;
   int n, local_n, rank, num_ranks;

   MPI_Init(&argc, &argv);
   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);
   MPI_Comm_rank(MPI_COMM_WORLD, &rank);

   a = atof(argv[1]);
   b = atof(argv[2]);
   n = atoi(argv[3]);

   h = (b - a) / n;
   local_n = n / num_ranks;
   local_a = a + rank * local_n * h;
   local_b = local_a + local_n * h;
   integral = trapezoid(local_a, local_b, local_n, h);

   if (rank == 0) {
      results = (double *) malloc(num_ranks * sizeof(double));
   }

   MPI_Gather(&integral, 1, MPI_DOUBLE, results, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);

   if (rank == 0) {
      for (int i = 0; i < num_ranks; i++) {
         sum += results[i];
      }
      printf("%lf\n", sum);
   }
   MPI_Finalize();
}

double trapezoid(double a, double b, int n, double h) {
   double result;
   double x = a;
   result = (f(a) + f(b)) / 2.0;
   for (int i = 1; i < n; i++) {
      x = x + h;
      result += f(x);
   }
   return result * h;
}

double f(double x) {
   return 4.0 / (1 + x * x);
}
\end{Verbatim}


Or you can you is to implement less trivial communication patterns (allreduce example or something more complex?)


\section{Conclusion, further reading, next steps, ...}

\Todo mention somewhere bundled example and tests that could provide inspiration :)

Full \verb|haskell-mpi| sources\footnote{github, cabal source
  haskell-mpi} also contain a comprehensive testsuite and a set of
examples, which demonstrate the use of all of the supported functions.

\Todo mention Well-Typed

\section{Appendix A: Installation and tested MPI implementations}
\label{appendix-A}
We tested the code on MPICH 1.2.x and 1.4.x, OpenMPI x.y.z. Amount of implementation-specific code is minimal and there is a good chance that all the other MPI implementations would be supported out of the box.

To install, try ``cabal install haskell-mpi''. If that fails to find MPI include/library files in system-wide directories, try ...
\Todo provide installation cmdline

\bibliography{haskell-mpi}

\end{document}
